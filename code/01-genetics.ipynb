{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  FrozenLake\n",
    "Today you are going to learn how to survive walking over the (virtual) frozen lake through discrete optimization.\n",
    "\n",
    "<img src=\"http://vignette2.wikia.nocookie.net/riseoftheguardians/images/4/4c/Jack's_little_sister_on_the_ice.jpg/revision/latest?cb=20141218030206\" alt=\"a random image to attract attention\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In google collab, uncomment this:\n",
    "# !wget https://bit.ly/2FMJP5K -O setup.py && bash setup.py\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "# import os\n",
    "# if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "#     !bash ../xvfb start\n",
    "#     %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "#create a single game instance\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "#start new game\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the game state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### legend\n",
    "\n",
    "![img](https://cdn-images-1.medium.com/max/800/1*MCjDzR-wfMMkS0rPqXSmKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym interface\n",
    "\n",
    "The three main methods of an environment are\n",
    "* __reset()__ - reset environment to initial state, _return first observation_\n",
    "* __render()__ - show current environment state (a more colorful version :) )\n",
    "* __step(a)__ - commit action __a__ and return (new observation, reward, is done, info)\n",
    " * _new observation_ - an observation right after commiting the action __a__\n",
    " * _reward_ - a number representing your reward for commiting action __a__\n",
    " * _is done_ - True if the MDP has just finished, False if still in progress\n",
    " * _info_ - some auxilary stuff about what just happened. Ignore it for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"initial observation code:\", env.reset())\n",
    "print('printing observation:')\n",
    "env.render()\n",
    "print(\"observations:\", env.observation_space, 'n=', env.observation_space.n)\n",
    "print(\"actions:\", env.action_space, 'n=', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"taking action 2 (right)\")\n",
    "new_obs, reward, is_done, _ = env.step(2)\n",
    "print(\"new observation code:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"is game over?:\", is_done)\n",
    "print(\"printing new state:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_i = {\n",
    "    'left':0,\n",
    "    'down':1,\n",
    "    'right':2,\n",
    "    'up':3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with it\n",
    "* Try walking 5 steps without falling to the (H)ole\n",
    " * Bonus quest - get to the (G)oal\n",
    "* Sometimes your actions will not be executed properly due to slipping over ice\n",
    "* If you fall, call __env.reset()__ to restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(action_to_i['up'])\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "* The environment has a 4x4 grid of states (16 total), they are indexed from 0 to 15\n",
    "* From each states there are 4 actions (left,down,right,up), indexed from 0 to 3\n",
    "\n",
    "We need to define agent's policy of picking actions given states. Since we have only 16 disttinct states and 4 actions, we can just store the action for each state in an array.\n",
    "\n",
    "This basically means that any array of 16 integers from 0 to 3 makes a policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_policy():\n",
    "    \"\"\"\n",
    "    Build a numpy array representing agent policy.\n",
    "    This array must have one element per each of 16 environment states.\n",
    "    Element must be an integer from 0 to 3, representing action\n",
    "    to take from that state.\n",
    "    \"\"\"\n",
    "    # policy = ...\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [get_random_policy() for i in range(10**4)]\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should match n_actions-1'\n",
    "action_probas = np.unique(policies, return_counts=True)[-1] /10**4. /n_states\n",
    "print(\"Action frequencies over 10^4 samples:\",action_probas)\n",
    "assert np.allclose(action_probas, [1. / n_actions] * n_actions, atol=0.05), \"The policies aren't uniformly random (maybe it's just an extremely bad luck)\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate!\n",
    "* Implement a simple function that runs one game and returns the total reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_reward(env, policy, t_max=100):\n",
    "    \"\"\"\n",
    "    Interact with an environment, return sum of all rewards.\n",
    "    If game doesn't end on t_max (e.g. agent walks into a wall), \n",
    "    force end the game and return whatever reward you got so far.\n",
    "    Tip: see signature of env.step(...) method above.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i in range(t_max):\n",
    "        # action = ...\n",
    "        s, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"generating 10^3 sessions...\")\n",
    "rewards = [sample_reward(env,get_random_policy()) for _ in range(10**3)]\n",
    "assert all([type(r) in (int, float) for r in rewards]), 'sample_reward must return a single number'\n",
    "assert all([0 <= r <= 1 for r in rewards]), 'total rewards should be between 0 and 1 for frozenlake (if solving taxi, delete this line)'\n",
    "print(\"Looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(policy, n_times=100):\n",
    "    \"\"\"Run several evaluations and average the score the policy gets.\"\"\"\n",
    "    # avg_reward = ...\n",
    "    return avg_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    \"\"\"a function that displays a policy in a human-readable way.\"\"\"\n",
    "    lake = \"SFFFFHFHFFFHHFFG\"\n",
    "    assert env.spec.id == \"FrozenLake-v0\", \"this function only works with frozenlake 4x4\"\n",
    "    \n",
    "    # where to move from each tile\n",
    "    arrows = ['<v>^'[a] for a in policy]\n",
    "    \n",
    "    #draw arrows above S and F only\n",
    "    signs = [arrow if tile in \"SF\" else tile for arrow, tile in zip(arrows, lake)]\n",
    "    \n",
    "    for i in range(0, 16, 4):\n",
    "        print(' '.join(signs[i:i+4]))\n",
    "\n",
    "print(\"random policy:\")\n",
    "print_policy(get_random_policy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_policy = None\n",
    "best_score = -float('inf')\n",
    "\n",
    "from tqdm import tqdm\n",
    "tr = tqdm(range(int(1e4)))\n",
    "for i in tr:\n",
    "    policy = get_random_policy()\n",
    "    score = evaluate(policy)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_policy = policy\n",
    "    tr.set_postfix({\"best score:\": best_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II Genetic algorithm\n",
    "\n",
    "The next task is to devise some more effecient way to perform policy search.\n",
    "We'll do that with a bare-bones evolutionary algorithm.\n",
    "[unless you're feeling masochistic and wish to do something entirely different which is bonus points if it works]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(policy1, policy2, p=0.5, prioritize=False):\n",
    "    \"\"\"\n",
    "    for each state, with probability p take action from policy1, else policy2\n",
    "    \"\"\"\n",
    "    if prioritize:\n",
    "        # wait for part II - moar\n",
    "        pass\n",
    "    # policy = ...\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutation(policy, p=0.1):\n",
    "    \"\"\"\n",
    "    for each state, with probability p replace action with random action\n",
    "    Tip: mutation can be written as crossover with random policy\n",
    "    \"\"\"\n",
    "    # new_policy = ...\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "policies = [\n",
    "    crossover(get_random_policy(), get_random_policy()) \n",
    "    for i in range(10**4)]\n",
    "\n",
    "assert all([len(p) == n_states for p in policies]), 'policy length should always be 16'\n",
    "assert np.min(policies) == 0, 'minimal action id should be 0'\n",
    "assert np.max(policies) == n_actions-1, 'maximal action id should be n_actions-1'\n",
    "\n",
    "assert any([\n",
    "    np.mean(crossover(np.zeros(n_states), np.ones(n_states))) not in (0, 1)\n",
    "    for _ in range(100)]), \"Make sure your crossover changes each action independently\"\n",
    "print(\"Seems fine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 20 #how many cycles to make\n",
    "pool_size = 100 #how many policies to maintain\n",
    "n_crossovers = 50 #how many crossovers to make on each step\n",
    "n_mutations = 50 #how many mutations to make on each tick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initializing...\")\n",
    "pool = [get_random_policy() for _ in range(pool_size)]\n",
    "pool_scores = list(map(evaluate, pool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(pool) == type(pool_scores) == list\n",
    "assert len(pool) == len(pool_scores) == pool_size\n",
    "assert all([type(score) in (float, int) for score in pool_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "tr = tqdm(range(n_epochs))\n",
    "for epoch in tr:\n",
    "#     print(\"Epoch %s:\"%epoch)\n",
    "    crossovered = [\n",
    "        crossover(random.choice(pool), random.choice(pool)) \n",
    "        for _ in range(n_crossovers)]\n",
    "    mutated = [\n",
    "        mutation(random.choice(pool)) \n",
    "        for _ in range(n_mutations)]\n",
    "    \n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "    \n",
    "    # add new policies to the pool\n",
    "    # pool = ...\n",
    "    # pool_scores = ...\n",
    "    \n",
    "    # select pool_size best policies\n",
    "    # selected_indices = ...\n",
    "    # pool = ...\n",
    "    # pool_scores = ...\n",
    "\n",
    "    # print the best policy so far (last in ascending score order)\n",
    "    tr.set_postfix({\"best score:\": pool_scores[-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## moar\n",
    "\n",
    "The parameters of the genetic algorithm aren't optimal, try to find something better. (size, crossovers and mutations)\n",
    "\n",
    "Try alternative crossover and mutation strategies\n",
    "* prioritize crossover for higher-scorers?\n",
    "* try to select a more diverse pool, not just best scorers?\n",
    "* Just tune the f*cking probabilities.\n",
    "\n",
    "See which combination works best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(policy1, policy2, p=0.5, prioritize=False):\n",
    "    \"\"\"\n",
    "    for each state, with probability p take action from policy1, else policy2\n",
    "    \"\"\"\n",
    "    if prioritize:\n",
    "        # the time has come\n",
    "        pass\n",
    "    # policy = ...\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "tr = tqdm(range(n_epochs))\n",
    "for epoch in tr:\n",
    "#     print(\"Epoch %s:\"%epoch)\n",
    "    crossovered = [\n",
    "        crossover(\n",
    "            random.choice(pool), \n",
    "            random.choice(pool), \n",
    "            prioritize=True) \n",
    "        for _ in range(n_crossovers)]\n",
    "    mutated = [\n",
    "        mutation(random.choice(pool)) \n",
    "        for _ in range(n_mutations)]\n",
    "    \n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "    \n",
    "    # add new policies to the pool\n",
    "    # pool = ...\n",
    "    # pool_scores = ...\n",
    "    \n",
    "    # select pool_size best policies\n",
    "    # selected_indices = ...\n",
    "    # pool = ...\n",
    "    # pool_scores = ...\n",
    "\n",
    "    # print the best policy so far (last in ascending score order)\n",
    "    tr.set_postfix({\"best score:\": pool_scores[-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** Part III\n",
    "\n",
    "The frozenlake problem above is just too simple: you can beat it even with a random policy search. Go solve something more complicated. \n",
    "\n",
    "__FrozenLake8x8-v0__ - frozenlake big brother\n",
    "\n",
    "\n",
    "### Some tips:\n",
    "* Random policy search is worth trying as a sanity check, but in general you should expect the genetic algorithm (or anything you devised in it's place) to fare much better that random.\n",
    "* While _it's okay to adapt the tabs above to your chosen env_, make sure you didn't hard-code any constants there (e.g. 16 states or 4 actions).\n",
    "* `print_policy` function was built for the frozenlake-v0 env so it will break on any other env. You could simply ignore it OR write your own visualizer for bonus points.\n",
    "* in function `sample_reward`, __make sure t_max steps is enough to solve the environment__ even if agent is sometimes acting suboptimally. To estimate that, run several sessions without time limit and measure their length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a single game instance\n",
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "\n",
    "#start new game\n",
    "env.reset()\n",
    "\n",
    "# display the game state\n",
    "env.render()\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20 #how many cycles to make\n",
    "pool_size = 100 #how many policies to maintain\n",
    "n_crossovers = 50 #how many crossovers to make on each step\n",
    "n_mutations = 50 #how many mutations to make on each tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initializing...\")\n",
    "pool = [get_random_policy() for _ in range(pool_size)]\n",
    "pool_scores = list(map(evaluate, pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "tr = tqdm(range(n_epochs))\n",
    "for epoch in tr:\n",
    "#     print(\"Epoch %s:\"%epoch)\n",
    "    crossovered = [\n",
    "        crossover(\n",
    "            random.choice(pool), \n",
    "            random.choice(pool), \n",
    "            prioritize=True) \n",
    "        for _ in range(n_crossovers)]\n",
    "    mutated = [\n",
    "        mutation(random.choice(pool)) \n",
    "        for _ in range(n_mutations)]\n",
    "    \n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "    \n",
    "    # add new policies to the pool\n",
    "    # pool = ...\n",
    "    # pool_scores = ...\n",
    "    \n",
    "    # select pool_size best policies\n",
    "    # selected_indices = ...\n",
    "    # pool = ...\n",
    "    # pool_scores = ...\n",
    "\n",
    "    # print the best policy so far (last in ascending score order)\n",
    "    tr.set_postfix({\"best score:\": pool_scores[-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# moar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_reward(env, policy, t_max=200):\n",
    "    \"\"\"\n",
    "    Interact with an environment, return sum of all rewards.\n",
    "    If game doesn't end on t_max (e.g. agent walks into a wall), \n",
    "    force end the game and return whatever reward you got so far.\n",
    "    Tip: see signature of env.step(...) method above.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for i in range(t_max):\n",
    "        action = policy[s]\n",
    "        s, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a single game instance\n",
    "env = gym.make(\"FrozenLake8x8-v0\")\n",
    "\n",
    "# start new game\n",
    "env.reset()\n",
    "\n",
    "# display the game state\n",
    "env.render()\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50 #how many cycles to make\n",
    "pool_size = 200 #how many policies to maintain\n",
    "n_crossovers = 100 #how many crossovers to make on each step\n",
    "n_mutations = 100 #how many mutations to make on each tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"initializing...\")\n",
    "pool = [get_random_policy() for _ in range(pool_size)]\n",
    "pool_scores = list(map(evaluate, pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "tr = tqdm(range(n_epochs))\n",
    "for epoch in tr:\n",
    "#     print(\"Epoch %s:\"%epoch)\n",
    "    crossovered = [\n",
    "        crossover(\n",
    "            random.choice(pool), \n",
    "            random.choice(pool), \n",
    "            prioritize=True) \n",
    "        for _ in range(n_crossovers)]\n",
    "    mutated = [\n",
    "        mutation(random.choice(pool)) \n",
    "        for _ in range(n_mutations)]\n",
    "    \n",
    "    assert type(crossovered) == type(mutated) == list\n",
    "    \n",
    "    # add new policies to the pool\n",
    "    # pool = ...\n",
    "    # pool_scores = ...\n",
    "    \n",
    "    # select pool_size best policies\n",
    "    # selected_indices = ...\n",
    "    # pool = ...\n",
    "    # pool_scores = ...\n",
    "\n",
    "    # print the best policy so far (last in ascending score order)\n",
    "    tr.set_postfix({\"best score:\": pool_scores[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
