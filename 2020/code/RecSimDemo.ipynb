{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plt.rcParams['axes.grid'] = True\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "from recsim import document, user\n",
    "from recsim.choice_model import AbstractChoiceModel\n",
    "from recsim.simulator import recsim_gym, environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsim_exp import GaussNoise, WolpertingerRecommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecSim environment\n",
    "\n",
    "In this tutorial we will break a RecSim environment down into its basic components. \n",
    "![Detailed view of RecSim](https://github.com/google-research/recsim/blob/master/recsim/colab/figures/simulator.png?raw=true)\n",
    "\n",
    "The green and blue blocks in the above diagram constitute the classes that need to be implemented within a RecSim environment. The goal of this tutorial is to explain the purpose of these blocks and how they come together in a simulation.  In the process, we will go over an example end-to-end implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "A single step of a RecSim simulation can be summarized roughly as follows:\n",
    "\n",
    "\n",
    "1.   the document database provides a corpus of *D* documents to the recommender. This could be a different set at each step (e.g., sampled, or produced by some \"candidate generation\" process), or fixed throughout the simulation. Each document is represented by a list of features. In a fully observable situation, the recommender observes all features of each document that impact the user's state and choice of document (and other aspects of the user's response), but this need not be the case in general. (In fact, most interesting scenarios involve latent features.)\n",
    "2.   The recommender observes the *D* documents (and their features) together with the user's response to the last recommendation. It then makes a selection (possibly ordered) of *k* documents and presents them to the user. The ordering may or may not impact the user choice or user state, depending on our simulation goals.\n",
    "3.   The user examines the list of documents and makes a choice of one document. Note that not consuming any of the documents is also a valid choice. This leads to a transition in the user's state. Finally the user emits an observation, which the recommender observes at the next iteration. The observation generally includes (noisy) information about the user's reaction to the content and potentially clues about the user's latent state. Typically, the user's state is not fully revealed. \n",
    "\n",
    "If we examine at the diagram above carefully, we notice that the flow of information along arcs is acyclic---a RecSim environment is a dynamic Bayesian network (DBN), where the various boxes represent conditional probability distributions. We will now define a simple simulation problem and implement it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DOC_NUM = 10\n",
    "P_EXIT_ACCEPTED = 0.1\n",
    "P_EXIT_NOT_ACCEPTED = 0.2\n",
    "\n",
    "# let's define a matrix W for simulation of users' respose\n",
    "# (based on the section 7.3 of the paper https://arxiv.org/pdf/1512.07679.pdf)\n",
    "# W_ij defines the probability that a user will accept recommendation j\n",
    "# given that he is consuming item i at the moment\n",
    "\n",
    "np.random.seed(SEED)\n",
    "W = (np.ones((DOC_NUM, DOC_NUM)) - np.eye(DOC_NUM)) * \\\n",
    "     np.random.uniform(0.0, P_EXIT_NOT_ACCEPTED, (DOC_NUM, DOC_NUM)) + \\\n",
    "     np.diag(np.random.uniform(1.0 - P_EXIT_ACCEPTED, 1.0, DOC_NUM))\n",
    "W = W[:, np.random.permutation(DOC_NUM)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(document.AbstractDocument):\n",
    "\n",
    "    def __init__(self, doc_id):\n",
    "        super(Document, self).__init__(doc_id)\n",
    "\n",
    "    def create_observation(self):\n",
    "        return (self._doc_id,)\n",
    "\n",
    "    @staticmethod\n",
    "    def observation_space():\n",
    "        return spaces.Discrete(DOC_NUM)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Document #{}\".format(self._doc_id)\n",
    "\n",
    "\n",
    "class DocumentSampler(document.AbstractDocumentSampler):\n",
    "\n",
    "    def __init__(self, doc_ctor=Document):\n",
    "        super(DocumentSampler, self).__init__(doc_ctor)\n",
    "        self._doc_count = 0\n",
    "\n",
    "    def sample_document(self):\n",
    "        doc = self._doc_ctor(self._doc_count % DOC_NUM)\n",
    "        self._doc_count += 1\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserState(user.AbstractUserState):\n",
    "\n",
    "    def __init__(self, user_id, current, active_session=True):\n",
    "        self.user_id = user_id\n",
    "        self.current = current\n",
    "        self.active_session = active_session\n",
    "\n",
    "    def create_observation(self):\n",
    "        return (self.current,)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"User #{}\".format(self.user_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def observation_space():\n",
    "        return spaces.Discrete(DOC_NUM)\n",
    "\n",
    "    def score_document(self, doc_obs):\n",
    "        return W[self.current, doc_obs[0]]\n",
    "\n",
    "\n",
    "class StaticUserSampler(user.AbstractUserSampler):\n",
    "\n",
    "    def __init__(self, user_ctor=UserState):\n",
    "        super(StaticUserSampler, self).__init__(user_ctor)\n",
    "        self.user_count = 0\n",
    "\n",
    "    def sample_user(self):\n",
    "        self.user_count += 1\n",
    "        sampled_user = self._user_ctor(\n",
    "            self.user_count, np.random.randint(DOC_NUM))\n",
    "        return sampled_user\n",
    "\n",
    "\n",
    "class Response(user.AbstractResponse):\n",
    "\n",
    "    def __init__(self, accept=False):\n",
    "        self.accept = accept\n",
    "\n",
    "    def create_observation(self):\n",
    "        return (int(self.accept),)\n",
    "\n",
    "    @classmethod\n",
    "    def response_space(cls):\n",
    "        return spaces.Discrete(2)\n",
    "\n",
    "\n",
    "class UserChoiceModel(AbstractChoiceModel):\n",
    "    def __init__(self):\n",
    "        super(UserChoiceModel, self).__init__()\n",
    "        self._score_no_click = P_EXIT_ACCEPTED\n",
    "\n",
    "    def score_documents(self, user_state, doc_obs):\n",
    "        if len(doc_obs) != 1:\n",
    "            raise ValueError(\n",
    "                \"Expecting single document, but got: {}\".format(doc_obs))\n",
    "        self._scores = np.array(\n",
    "            [user_state.score_document(doc) for doc in doc_obs])\n",
    "\n",
    "    def choose_item(self):\n",
    "        if np.random.random() < self.scores[0]:\n",
    "            return 0\n",
    "\n",
    "\n",
    "class UserModel(user.AbstractUserModel):\n",
    "    def __init__(self):\n",
    "        super(UserModel, self).__init__(Response, StaticUserSampler(), 1)\n",
    "        self.choice_model = UserChoiceModel()\n",
    "\n",
    "    def simulate_response(self, slate_documents):\n",
    "        if len(slate_documents) != 1:\n",
    "            raise ValueError(\"Expecting single document, but got: {}\".format(\n",
    "                slate_documents))\n",
    "\n",
    "        responses = [self._response_model_ctor() for _ in slate_documents]\n",
    "\n",
    "        self.choice_model.score_documents(\n",
    "            self._user_state,\n",
    "            [doc.create_observation() for doc in slate_documents]\n",
    "        )\n",
    "        selected_index = self.choice_model.choose_item()\n",
    "\n",
    "        if selected_index is not None:\n",
    "            responses[selected_index].accept = True\n",
    "\n",
    "        return responses\n",
    "\n",
    "    def update_state(self, slate_documents, responses):\n",
    "        if len(slate_documents) != 1:\n",
    "            raise ValueError(\n",
    "                f\"Expecting single document, but got: {slate_documents}\"\n",
    "            )\n",
    "\n",
    "        response = responses[0]\n",
    "        doc = slate_documents[0]\n",
    "        if response.accept:\n",
    "            self._user_state.current = doc.doc_id()\n",
    "            self._user_state.active_session = bool(\n",
    "                np.random.binomial(1, 1 - P_EXIT_ACCEPTED))\n",
    "        else:\n",
    "            self._user_state.current = np.random.choice(DOC_NUM)\n",
    "            self._user_state.active_session = bool(\n",
    "                np.random.binomial(1, 1 - P_EXIT_NOT_ACCEPTED))\n",
    "\n",
    "    def is_terminal(self):\n",
    "        \"\"\"Returns a boolean indicating if the session is over.\"\"\"\n",
    "        return not self._user_state.active_session\n",
    "\n",
    "\n",
    "def clicked_reward(responses):\n",
    "    reward = 0.0\n",
    "    for response in responses:\n",
    "        if response.accept:\n",
    "            reward += 1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = recsim_gym.RecSimGymEnv(\n",
    "        environment.Environment(\n",
    "            UserModel(), \n",
    "            DocumentSampler(), \n",
    "            DOC_NUM, \n",
    "            1, \n",
    "            resample_documents=False\n",
    "        ),\n",
    "        clicked_reward\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RecSim Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For solving of this toy environment we'll try using a variant of DDPG algorithm for discrete actions.\n",
    "We need to embed our discrete actions into continuous space to use DDPG (it outputs \"proto action\").\n",
    "Then we choose k nearest embedded actions and take the action with maximum Q value.\n",
    "Thus, we can avoid taking maximum over all the action space as in DQN, which can be too large in case of RecSys.\n",
    "In our example embeddings are just one hot vectors. Therefore the nearest neighbour is argmax of proto action.\n",
    "\n",
    "<img src=\"../presets/wolpertinger_scheme.png\" width=400 height=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(\n",
    "    env, \n",
    "    agent, \n",
    "    num_steps: int = int(3e3), \n",
    "    log_every: int = int(1e3)\n",
    "):\n",
    "    reward_history = []\n",
    "    step, episode = 1, 1\n",
    "\n",
    "    observation = env.reset()\n",
    "    while step < num_steps:\n",
    "        action = agent.begin_episode(observation)\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "\n",
    "            if step % log_every == 0:\n",
    "                print(step, np.mean(reward_history[-50:]))\n",
    "            step += 1\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                action = agent.step(reward, observation)\n",
    "\n",
    "        agent.end_episode(reward, observation)\n",
    "        reward_history.append(episode_reward)\n",
    "\n",
    "    return reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"action_dim\": DOC_NUM,\n",
    "    \"state_dim\": DOC_NUM,\n",
    "    \"noise\": GaussNoise(sigma=0.05),\n",
    "    \"critic_lr\": 1e-3,\n",
    "    \"actor_lr\": 1e-3,\n",
    "    \"tau\": 1e-3,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": int(1e4),\n",
    "    \"gamma\": 0.8,\n",
    "    \"actor_weight_decay\": 0.0001,\n",
    "    \"critic_weight_decay\": 0.001,\n",
    "    \"eps\": 1e-2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "agent = WolpertingerRecommender(\n",
    "    env=env, \n",
    "    k_ratio=0.33, \n",
    "    **parameters\n",
    ")\n",
    "reward_history = run_agent(env, agent)\n",
    "plt.plot(pd.Series(reward_history).rolling(50).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_qvalues = np.hstack([\n",
    "    agent.agent.predict_qvalues(i) for i in range(DOC_NUM)\n",
    "]).T\n",
    "predicted_actions = np.vstack([\n",
    "    agent.agent.predict_action(np.eye(DOC_NUM)[i], with_noise=False)\n",
    "    for i in range(DOC_NUM)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned Qvalues \n",
    "plt.subplots(figsize=predicted_qvalues.shape)\n",
    "sns.heatmap(predicted_qvalues.round(3), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learned actions (aka policy)\n",
    "plt.subplots(figsize=predicted_qvalues.shape)\n",
    "sns.heatmap(predicted_actions.round(3), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true actions (aka policy)\n",
    "plt.subplots(figsize=predicted_qvalues.shape)\n",
    "sns.heatmap(W, annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recsim.agent import AbstractEpisodicRecommenderAgent\n",
    "\n",
    "class OptimalRecommender(AbstractEpisodicRecommenderAgent):\n",
    "\n",
    "    def __init__(self, environment, W):\n",
    "        super().__init__(environment.action_space)\n",
    "        self._observation_space = environment.observation_space\n",
    "        self._W = W\n",
    "\n",
    "    def _extract_state(self, observation):\n",
    "        user_space = self._observation_space.spaces[\"user\"]\n",
    "        return spaces.flatten(user_space, observation[\"user\"])\n",
    "\n",
    "    def step(self, reward, observation):\n",
    "        state = self._extract_state(observation)\n",
    "        return [self._W[state.argmax(), :].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "agent = OptimalRecommender(env, W)\n",
    "\n",
    "reward_history = run_agent(env, agent)\n",
    "plt.plot(pd.Series(reward_history).rolling(50).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from recsim.agents.tabular_q_agent import TabularQAgent\n",
    "\n",
    "# env = make_env()\n",
    "# q_agent = TabularQAgent(env.observation_space, env.action_space)\n",
    "\n",
    "# reward_history = run_agent(env, agent)\n",
    "# plt.plot(pd.Series(reward_history).rolling(50).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
